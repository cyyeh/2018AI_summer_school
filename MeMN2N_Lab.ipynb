{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MeMN2N_Lab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/cyyeh/2018AI_summer_school/blob/master/MeMN2N_Lab.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "D1uUnIqMgVwi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# End-To-End Memory Networks\n",
        "## Tutorial in TensorFlow\n",
        "---\n",
        "This tutorial will introduce the implementation of  <a href=\"https://arxiv.org/pdf/1503.08895.pdf\">End-To-End Memory Networks</a>, and is based on the github repository - <a href=\"https://github.com/domluna/memn2n\">memn2n</a>. \n",
        "\n",
        "We will cover synthetic question and answering experiments on bAbI dataset. \n",
        "\n",
        "### bAbI Dataset\n",
        "\n",
        "<a href=\"https://arxiv.org/pdf/1502.05698.pdf\">bAbI dataset</a> is a set of proxy tasks that evaluate reading comprehension via question answering. Their tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many\n",
        "more.\n",
        "\n",
        "The following is an example in one of 20 bAbI task:\n",
        "\n",
        "> 1 Mary moved to the bathroom.  \n",
        "2 John went to the hallway.  \n",
        "3 Where is Mary? \tbathroom\t1  \n",
        "4 Daniel went back to the hallway.  \n",
        "5 Sandra moved to the garden.  \n",
        "6 Where is Daniel? \thallway\t4  \n",
        "7 John moved to the office.  \n",
        "8 Sandra journeyed to the bathroom.  \n",
        "9 Where is Daniel? \thallway\t4  \n",
        "\n",
        "\n",
        "### Introduction to End-To-End Memory Networks\n",
        "\n",
        "![MeMN2N Picture](https://camo.githubusercontent.com/ba1c7dbbccc5dd51d4a76cc6ef849bca65a9bf4d/687474703a2f2f692e696d6775722e636f6d2f6e7638394a4c632e706e67 \"MeMN2N\")\n",
        "Illustration of End-To-End Memory Networks Architecture.\n",
        "\n",
        "#### Model Inputs and Outputs\n",
        "---\n",
        "The model takes a discrete set of inputs $x_1, ..., x_n$ that are to be stored in the memory, a query $q$, and outputs an answer $a$. Each of the $x_i$, $q$, and $a$ contains symbols coming from a dictionary with $V$ words. The model writes all $x$ to the memory up to a fixed buffer size, and then finds a continuous representation for the $x$ and $q$. The continuous representation is then processed via multiple hops to output $a$. \n",
        "\n",
        "Note that $x_i$, $q$, and $a$ can be seen as vectors of dimension $V$.\n",
        "#### Input memory representation\n",
        "---\n",
        "1. Given $\\{x_i\\}$ and *embedding matrix $A$* of size $d\\times V$,  the **memory vectors $m_i$** is derived from $m_i = A x_i$ of dimension $d$.\n",
        "\n",
        "2. Given $q$ and *embedding matrix $B$* of size $d\\times V$, the **inital internal state $u$** is obtained from $u = B q$ of dimension $d$.\n",
        "\n",
        "3. The **match probability vector $p_i$** that measure the match confidence between $u$ and $m_i$ is aquired from $p_i = Softmax(u^T m_i)$, where $Softmax(z_i) = e^{z_i} / \\sum_j e^{z_j}$.\n",
        "\n",
        "#### Output memory representation\n",
        "---\n",
        "1. Given $\\{x_i\\}$ and *embedding matrix $C$* of size $d\\times V$,  the **output vectors $c_i$** is derived from $c_i = C x_i$ of dimension $d$.\n",
        "\n",
        "2. The **response vector $o$** is then a sum over the transformed inputs $c_i$, weighted by the probability vector from the input: $o = \\sum_i p_i c_i$.\n",
        "\n",
        "3. The **final prediction $\\hat{a} = Softmax(W(o+u))$**.\n",
        "\n",
        "#### Multiple Layers\n",
        "---\n",
        "The End-To-End Memory Networks can be extended to handle $K$ hop operations.  The memory layers are stacked $K$ times in the following way:\n",
        "\n",
        "* $u^{k+1} = u^k + o^k$\n",
        "* Each layer has its own embedding matrices $A^k$, $C^k$, used to embed the inputs $\\{x_i\\}$\n",
        "* $\\hat{a}= Softmax(W u^{K+1}) = Softmax(W(o^K + u^K))$\n",
        "\n",
        "Note that superscipt of $k$ means from layer k, and $K$ means from last layer. In the original paper, there are two types of weight tying within model:\n",
        "\n",
        "1. **Adjacent**: the output embedding for one layer is the input embedding for the one above, i.e. $A^{k+1} = C^k$. They also constrain (a) the answer prediction matrix to be the same as the final output embedding, i.e. $W^T = C^K$, and (b) the question embedding to match the input\n",
        "embedding of the first layer, i.e. $B = A^1$.\n",
        "2. **Layer-wise (RNN-like)**: the input and output embeddings are the same across different\n",
        "layers, i.e. $A^1 = A^2 = ... = A^K$ and $C^1 = C^2 = ... = C^K$. They have found it useful to\n",
        "add a linear mapping $H$ to the update of $u$ between hops; that is, $u^{k+1} = H u^k + o^k$. This\n",
        "mapping is learnt along with the rest of the parameters and used throughout our experiments for layer-wise weight tying.\n",
        "\n",
        "#### Lab Experiment Setting\n",
        "---\n",
        "During training, all three embedding matrices $A$, $B$ and $C$, and W are jointly learned by minimizing a standard cross-entropy loss between $\\hat{a}$ and the true label $a$.\n",
        "\n",
        "In the following lab experiment, we will use the setting below:\n",
        "\n",
        "1. All experiments use a $K = 3$ hops model with the **adjacent weight sharing scheme**. For all tasks that output lists (i.e. the answers are multiple words), we take each possible combination of possible outputs and record them as a separate answer vocabulary word.\n",
        "2. **Position Encoding (PE)**:  \n",
        "$m_i = \\sum_j l_j \\cdot A x_{ij}$  \n",
        "$c_i = \\sum_j l_j \\cdot Cx_{ij}$  \n",
        "$u =\\sum_j l_j \\cdot Bq_j$  \n",
        "where $\\cdot$ is an element-wise multiplication. $l_j$ is a column vector of $j$th word with the structure $l_{kj} = 1 + 4 * (j-(d+1)/2)(k-(J+1)/2)/Jd$ (assuming 1-based indexing), with $J$ being the number of words in the sentence, and $d$ is the dimension of the embedding.\n",
        "3. **Temporal Encoding**: To enable our model to address notion of temporal context, we modify the memory vector so that $m_i = \\sum_j A x_{ij} + T_A(i)$, where $T_A(i)$ is the ith row of a special matrix $T_A$ that encodes temporal information. The output embedding is augmented in the same way with a matrix $T_C$ (e.g. $c_i = \\sum_j C x_{ij} + T_C(i)$). Both $T_A$ and $T_C$ are learned during training. They are also subject to the same sharing constraints as $A$ and $C$. Note that sentences are indexed in reverse order, reflecting their relative distance from the question so that $x_1$ is the last sentence of the story.\n",
        "\n",
        "### Download bAbI Dataset\n",
        "\n",
        "Run the code cell below to download and extract bAbI dataset."
      ]
    },
    {
      "metadata": {
        "id": "5k-DKzTZ-7Dm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "!wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2.tar.gz\n",
        "!tar xzf ./tasks_1-20_v1-2.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T44FXaBRjKNo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Library and Parameter Setting\n",
        "The library is imported and all hyper-parameters is defined here. In the following lab lecture, we will choose one of 20 bAbI tasks to demostrate the  network implementation and training process."
      ]
    },
    {
      "metadata": {
        "id": "7eb4fSNhgCpR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import model_selection, metrics\n",
        "from itertools import chain\n",
        "from six.moves import range, reduce\n",
        "import re\n",
        "import os\n",
        "\n",
        "class Parameter(object):\n",
        "    learning_rate = 0.01     # Learning rate for SGD.\n",
        "    anneal_rate = 25         # Number of epochs between halving the learnign rate.\n",
        "    anneal_stop_epoch = 100  # Epoch number to end annealed lr schedule.\n",
        "    max_grad_norm = 40.0     # Clip gradients to this norm.\n",
        "    evaluation_interval = 10 # Evaluate and print results every x epochs\"\n",
        "    batch_size = 32          # Batch size for training.\n",
        "    hops = 3                 # Number of hops in the Memory Network.\n",
        "    epochs = 100             # Number of epochs to train for.\n",
        "    embedding_size = 20      # Embedding size for embedding matrices.\n",
        "    memory_size = 50         # Maximum size of memory.\n",
        "    task_id = 1              # bAbI task id, 1 <= id <= 20\n",
        "    random_state = None      # Random state.\n",
        "    name = 'MemN2N'          # Model name.\n",
        "    data_dir = './tasks_1-20_v1-2/en/' # Directory containing bAbI tasks\n",
        "\n",
        "param = Parameter()\n",
        "\n",
        "print(\"Started Task:\", param.task_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QPkq0Ns6FM3E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load Dataset\n",
        "1. ```load_task``` loads train and test data for  ```task_id``` th of bAbI tasks.\n",
        "2. ```parse_stories``` parse the content like:  \n",
        "`1 Mary moved to the bathroom.`  \n",
        "`2 John went to the hallway.`  \n",
        "`3 Where is Mary? \tbathroom\t1`  \n",
        "`4 Daniel went back to the hallway.`  \n",
        "`5 Sandra moved to the garden.`  \n",
        "`6 Where is Daniel? \thallway\t4`  \n",
        "`7 John moved to the office.`  \n",
        "`8 Sandra journeyed to the bathroom.`  \n",
        "`9 Where is Daniel? \thallway\t4`  \n",
        "One complete story corresponding queries and answers will cover line 1 to the previous line of next line 1. Each query has its answer and the supporting fact id in the current story. The output of ```parse_stories``` is ```[(substory, q, a),...]```."
      ]
    },
    {
      "metadata": {
        "id": "eBHod6PkFNhW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_task(data_dir, task_id, only_supporting=False):\n",
        "    '''Load the nth task. There are 20 tasks in total.\n",
        "    Returns a tuple containing the training and testing data for the task.\n",
        "    '''\n",
        "    assert task_id > 0 and task_id < 21\n",
        "\n",
        "    files = [os.path.join(data_dir, f) for f in os.listdir(data_dir)]\n",
        "    s = 'qa{}_'.format(task_id)\n",
        "    train_file = [f for f in files if s in f and 'train' in f][0]\n",
        "    test_file = [f for f in files if s in f and 'test' in f][0]\n",
        "    train_data = get_stories(train_file, only_supporting)\n",
        "    test_data = get_stories(test_file, only_supporting)\n",
        "    return train_data, test_data\n",
        "\n",
        "def get_stories(f, only_supporting=False):\n",
        "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
        "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
        "    '''\n",
        "    with open(f) as f:\n",
        "        return parse_stories(f.readlines(), only_supporting=only_supporting)\n",
        "\n",
        "def parse_stories(lines, only_supporting=False):\n",
        "    '''Parse stories provided in the bAbI tasks format\n",
        "    If only_supporting is true, only the sentences that support the answer are kept.\n",
        "    '''\n",
        "    data = []\n",
        "    story = []\n",
        "    for line in lines:\n",
        "        line = str.lower(line)\n",
        "        nid, line = line.split(' ', 1)\n",
        "        nid = int(nid)\n",
        "        if nid == 1:\n",
        "            story = []\n",
        "        if '\\t' in line: # question\n",
        "            q, a, supporting = line.split('\\t')\n",
        "            q = tokenize(q)\n",
        "            #a = tokenize(a)\n",
        "            # answer is one vocab word even if it's actually multiple words\n",
        "            a = [a]\n",
        "            substory = None\n",
        "\n",
        "            # remove question marks\n",
        "            if q[-1] == \"?\":\n",
        "                q = q[:-1]\n",
        "\n",
        "            if only_supporting:\n",
        "                # Only select the related substory\n",
        "                supporting = map(int, supporting.split())\n",
        "                substory = [story[i - 1] for i in supporting]\n",
        "            else:\n",
        "                # Provide all the substories\n",
        "                substory = [x for x in story if x]\n",
        "\n",
        "            data.append((substory, q, a))\n",
        "            story.append('')\n",
        "        else: # regular sentence\n",
        "            # remove periods\n",
        "            sent = tokenize(line)\n",
        "            if sent[-1] == \".\":\n",
        "                sent = sent[:-1]\n",
        "            story.append(sent)\n",
        "    return data\n",
        "    \n",
        "def tokenize(sent):\n",
        "    '''Return the tokens of a sentence including punctuation.\n",
        "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
        "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
        "    '''\n",
        "    return [x.strip() for x in re.split('\\W+', sent) if x.strip()]\n",
        "\n",
        "# task data\n",
        "train, test = load_task(param.data_dir, param.task_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k7W3tD-_jr77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Preprocessing\n",
        "1. Create vocabulary.\n",
        "2. Perform statistical analysis on data.\n"
      ]
    },
    {
      "metadata": {
        "id": "ycRfhqXs-ME6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = train + test\n",
        "\n",
        "vocab = sorted(reduce(lambda x, y: x | y, (set(list(chain.from_iterable(s)) + q + a) for s, q, a in data)))\n",
        "word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
        "\n",
        "max_story_size = max(map(len, (s for s, _, _ in data)))\n",
        "mean_story_size = int(np.mean([ len(s) for s, _, _ in data ]))\n",
        "sentence_size = max(map(len, chain.from_iterable(s for s, _, _ in data)))\n",
        "query_size = max(map(len, (q for _, q, _ in data)))\n",
        "memory_size = min(param.memory_size, max_story_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "No441iWBmxj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that in this implementation, we append **temporal encoding matrix** to **embedding matrix**. Since that, we add `time' words to the vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "CTvGQczkmxtx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Vocabulary size before adding time words\", len(word_idx))\n",
        "# Add time words/indexes\n",
        "for i in range(memory_size):\n",
        "    word_idx['time{}'.format(i+1)] = 'time{}'.format(i+1)\n",
        "\n",
        "vocab_size = len(word_idx) + 1 # +1 for nil word\n",
        "sentence_size = max(query_size, sentence_size) # for the position encoding\n",
        "sentence_size += 1  # +1 for time words\n",
        "\n",
        "print(\"Vocabulary size after adding time words\", len(word_idx))\n",
        "print(\"Longest sentence length\", sentence_size)\n",
        "print(\"Longest story length\", max_story_size)\n",
        "print(\"Average story length\", mean_story_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6VH-6lcEGe9b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encode Data\n",
        "```vectorize_data``` encodes stories, queries and answers.   \n",
        "For stories, we encode and pad lists of words into indices arrays and add `time' word index to the last of each sentence. Then, pad the arrays with zero to memory size.\n",
        "\n",
        "For queries, we also encode and pad lists of words into indices arrays. As for answers, we encode them into one hot vectors.\n",
        "\n",
        "Stories' shape = ```(num_queries, memory_size, sentence_size)```  \n",
        "Queries' shape = ```(num_queries, sentence_size)```  \n",
        "Answers' shape = ```(num_queries, vocab_size)```\n",
        "\n",
        "On line 23-24, the index order of sentence is reversed which corresponds to **temporal encoding** setting."
      ]
    },
    {
      "metadata": {
        "id": "DZHJcQx-Gew6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def vectorize_data(data, word_idx, sentence_size, memory_size):\n",
        "    \"\"\"\n",
        "    Vectorize stories and queries.\n",
        "    If a sentence length < sentence_size, the sentence will be padded with 0's.\n",
        "    If a story length < memory_size, the story will be padded with empty memories.\n",
        "    Empty memories are 1-D arrays of length sentence_size filled with 0's.\n",
        "    The answer array is returned as a one-hot encoding.\n",
        "    \"\"\"\n",
        "    S = []\n",
        "    Q = []\n",
        "    A = []\n",
        "    for story, query, answer in data:\n",
        "        ss = []\n",
        "        for i, sentence in enumerate(story, 1):\n",
        "            ls = max(0, sentence_size - len(sentence))\n",
        "            ss.append([word_idx[w] for w in sentence] + [0] * ls)\n",
        "\n",
        "        # take only the most recent sentences that fit in memory\n",
        "        ss = ss[::-1][:memory_size][::-1]\n",
        "\n",
        "        # Make the last word of each sentence the time 'word' which \n",
        "        # corresponds to vector of lookup table\n",
        "        for i in range(len(ss)):\n",
        "            ss[i][-1] = len(word_idx) - memory_size - i + len(ss)\n",
        "\n",
        "        # pad to memory_size\n",
        "        lm = max(0, memory_size - len(ss))\n",
        "        for _ in range(lm):\n",
        "            ss.append([0] * sentence_size)\n",
        "\n",
        "        lq = max(0, sentence_size - len(query))\n",
        "        q = [word_idx[w] for w in query] + [0] * lq\n",
        "\n",
        "        y = np.zeros(len(word_idx) + 1) # 0 is reserved for nil word\n",
        "        for a in answer:\n",
        "            y[word_idx[a]] = 1\n",
        "\n",
        "        S.append(ss)\n",
        "        Q.append(q)\n",
        "        A.append(y)\n",
        "    return np.array(S), np.array(Q), np.array(A)\n",
        "\n",
        "\n",
        "# train/validation/test sets\n",
        "S, Q, A = vectorize_data(train, word_idx, sentence_size, memory_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dwluGgpKH2AV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Split\n",
        "We split data into train, validation and test set. Note that train and validation set is split from data in the train file. Train set contains 90% of data and validation set contains 10 % of data."
      ]
    },
    {
      "metadata": {
        "id": "zHeHmc0bH2es",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "trainS, valS, trainQ, valQ, trainA, valA = model_selection.train_test_split(S, Q, A, test_size=.1, random_state=param.random_state)\n",
        "testS, testQ, testA = vectorize_data(test, word_idx, sentence_size, memory_size)\n",
        "\n",
        "print(testS[0])\n",
        "\n",
        "print(\"Training set shape\", trainS.shape)\n",
        "\n",
        "# params\n",
        "n_train = trainS.shape[0]\n",
        "n_test = testS.shape[0]\n",
        "n_val = valS.shape[0]\n",
        "\n",
        "print(\"Training Size\", n_train)\n",
        "print(\"Validation Size\", n_val)\n",
        "print(\"Testing Size\", n_test)\n",
        "\n",
        "train_labels = np.argmax(trainA, axis=1)\n",
        "test_labels = np.argmax(testA, axis=1)\n",
        "val_labels = np.argmax(valA, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CO9U0OlRI-sM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create Model\n",
        "---\n",
        "#### Build Inputs\n",
        "The model inputs include stories, queries, answers, and learning rate. From **Encode Data** section, we know that:  \n",
        "Stories' shape = ```(num_queries, memory_size, sentence_size)```  \n",
        "Queries' shape = ```(num_queries, sentence_size)```  \n",
        "Answers' shape = ```(num_queries, vocab_size)```\n",
        "\n",
        "In this section, we prepare our inputs by applying **tf.placeholder**. Be aware that input data contrains only vocabulary index. The data type should be integer."
      ]
    },
    {
      "metadata": {
        "id": "RDyAT7dbI-2G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(param.random_state)\n",
        "\n",
        "_stories = 'Your code here'\n",
        "_queries = 'Your code here'\n",
        "_answers = 'Your code here'\n",
        "_lr = tf.placeholder(tf.float32, [], name=\"learning_rate\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wCGW_YNDMw3X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Build Variables\n",
        "The model has weight variables, and we will define them here.  Recall that the input and output memory representation have embedding matrix $A$, $B$, $C$, and $W$. Since we are using **adjacent weight sharing scheme**,it should follow $A^{k+1} = C^k$, $W^T = C^K$, and $B = A^1$."
      ]
    },
    {
      "metadata": {
        "id": "zxL0FAdjMxUh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################################################################################\n",
        "# TODO:                                                                         #\n",
        "# Create weight variables for model. Please follow the description above.       #\n",
        "#################################################################################\n",
        "initializer=tf.random_normal_initializer(stddev=0.1)\n",
        "with tf.variable_scope(param.name):\n",
        "    nil_word_slot = tf.zeros([1, param.embedding_size])\n",
        "    # hint: create temporary tensor. nil word should be at index 0.\n",
        "    #       you can use tf.concat to concat random initialized weight matrx with nil word vector.\n",
        "    A = 'Your code here'\n",
        "    _C = 'Your code here'\n",
        "    \n",
        "    # hint: use A to create variable\n",
        "    A_1 = 'Your code here'\n",
        "\n",
        "    C = []\n",
        "\n",
        "    for hopn in range(param.hops):\n",
        "        with tf.variable_scope('hop_{}'.format(hopn)):\n",
        "            # hint: use _C to create variable\n",
        "            C.append('Your code here')\n",
        "\n",
        "_nil_vars = set([A_1.name] + [x.name for x in C])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9sfxQsqQq3u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Create Pipeline\n",
        "In this section, we will build up our model. Note that we have to consider **adjacent weight sharing scheme**.\n",
        "\n",
        "1. Use  **Position Encoding (PE)** to encode sentences.   \n",
        ">**Position Encoding (PE)**:  \n",
        "$m_i = \\sum_j l_j \\cdot A x_{ij}$  \n",
        "$c_i = \\sum_j l_j \\cdot Cx_{ij}$  \n",
        "$u =\\sum_j l_j \\cdot Bq_j$  \n",
        "where $\\cdot$ is an element-wise multiplication. $l_j$ is a column vector of $j$th word with the structure  \n",
        "$l_{kj} = 1 + 4 * (j-(d+1)/2)(k-(J+1)/2)/Jd$ (assuming 1-based indexing),  \n",
        "with $J$ being the number of words in the sentence, and $d$ is the dimension of the embedding.\n",
        "\n",
        "2. Obtain **inital internal state $u = \\sum_j l_j \\cdot Bq_j$**\n",
        "3. Obtain **memory vector $m_i = \\sum_j l_j \\cdot A x_{ij}$**.\n",
        "4. Obtain **output vector $c_i = \\sum_j l_j \\cdot Cx_{ij}$**.\n",
        "5. Obtain **reponse vector $o = \\sum_i p_i c_i$**.\n",
        "6. Repeat 2-4 for $K$ times.\n",
        "7. Obtain **final prediction $\\hat{a} = Softmax(W(o+u))$**.\n",
        "\n",
        "\n",
        "![MeMN2N Picture](https://camo.githubusercontent.com/ba1c7dbbccc5dd51d4a76cc6ef849bca65a9bf4d/687474703a2f2f692e696d6775722e636f6d2f6e7638394a4c632e706e67 \"MeMN2N\")"
      ]
    },
    {
      "metadata": {
        "id": "HpFVktanQrDI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def position_encoding(sentence_size, embedding_size):\n",
        "    \"\"\"\n",
        "    Position Encoding described in section 4.1 [1]\n",
        "    \"\"\"\n",
        "    #################################################################################\n",
        "    # TODO:                                                                         #\n",
        "    # Implementation of position encoding. Return the position encoding numpy array #\n",
        "    # with size (sentence_size, embedding_size).                                    #\n",
        "    #################################################################################\n",
        "    encoding = np.ones('Your code here', dtype=np.float32)\n",
        "    ls = sentence_size+1\n",
        "    le = embedding_size+1\n",
        "    for i in range(1, le):\n",
        "        for j in range(1, ls):\n",
        "            encoding[i-1, j-1] = 'Your code here'\n",
        "    encoding = 1 + 4 * 'Your code here' / 'Your code here' / 'Your code here'\n",
        "    # Make position encoding of time words identity to avoid modifying them \n",
        "    encoding[:, -1] = 1.0\n",
        "    return np.transpose(encoding)\n",
        "\n",
        "_PE = tf.constant(position_encoding(sentence_size, param.embedding_size), name=\"encoding\")\n",
        "\n",
        "with tf.variable_scope(param.name):\n",
        "    # Use A_1 for thee question embedding as per Adjacent Weight Sharing\n",
        "    # Use tf.nn.embedding_lookup to retrieve embedding vector\n",
        "    q_emb = 'Your code here'\n",
        "    # Remenber to multiply the embedding by position encodding and \n",
        "    # sum the embedding vectors over sentence dimension\n",
        "    u_0 = 'Your code here'\n",
        "    u = [u_0]\n",
        "\n",
        "    for hopn in range(param.hops):\n",
        "        if hopn == 0:\n",
        "            m_emb_A = 'Your code here'\n",
        "            m_A = 'Your code here'\n",
        "\n",
        "        else:\n",
        "            with tf.variable_scope('hop_{}'.format(hopn - 1)):\n",
        "                m_emb_A = 'Your code here'\n",
        "                m_A = 'Your code here'\n",
        "\n",
        "        # hack to get around no reduce_dot\n",
        "        u_temp = tf.transpose(tf.expand_dims(u[-1], -1), [0, 2, 1])\n",
        "        dotted = tf.reduce_sum(m_A * u_temp, 2)\n",
        "\n",
        "        # Calculate probabilities\n",
        "        probs = 'Your code here'\n",
        "\n",
        "        probs_temp = tf.transpose(tf.expand_dims(probs, -1), [0, 2, 1])\n",
        "        with tf.variable_scope('hop_{}'.format(hopn)):\n",
        "            m_emb_C = 'Your code here'\n",
        "        m_C = 'Your code here'\n",
        "\n",
        "        c_temp = tf.transpose(m_C, [0, 2, 1])\n",
        "        o_k = 'Your code here'\n",
        "\n",
        "        u_k = u[-1] + o_k\n",
        "\n",
        "        u.append(u_k)\n",
        "\n",
        "    # Use last C for output (transposed)\n",
        "    with tf.variable_scope('hop_{}'.format(param.hops)):\n",
        "        logits = 'Your code here'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "36hDWR3HRENU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Prepare Training\n"
      ]
    },
    {
      "metadata": {
        "id": "PkcPZa9EREY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def zero_nil_slot(t, name=None):\n",
        "    \"\"\"\n",
        "    Overwrites the nil_slot (first row) of the input Tensor with zeros.\n",
        "    The nil_slot is a dummy slot and should not be trained and influence\n",
        "    the training algorithm.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"zero_nil_slot\", name, [t]) as name:\n",
        "        t = tf.convert_to_tensor(t, name=\"t\")\n",
        "        s = tf.shape(t)[1]\n",
        "        z = tf.zeros(tf.stack([1, s]))\n",
        "        return tf.concat(axis=0, values=[z, tf.slice(t, [1, 0], [-1, -1])], name=name)\n",
        "\n",
        "def add_gradient_noise(t, stddev=1e-3, name=None):\n",
        "    \"\"\"\n",
        "    Adds gradient noise as described in http://arxiv.org/abs/1511.06807 [2].\n",
        "    The input Tensor `t` should be a gradient.\n",
        "    The output will be `t` + gaussian noise.\n",
        "    0.001 was said to be a good fixed value for memory networks [2].\n",
        "    \"\"\"\n",
        "    with tf.name_scope(\"add_gradient_noise\", name, [t, stddev]) as name:\n",
        "        t = tf.convert_to_tensor(t, name=\"t\")\n",
        "        gn = tf.random_normal(tf.shape(t), stddev=stddev)\n",
        "        return tf.add(t, gn, name=name)\n",
        "\n",
        "# optimizer\n",
        "opt = tf.train.GradientDescentOptimizer(learning_rate=_lr)\n",
        "# cross entropy\n",
        "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf.cast(_answers, tf.float32), name=\"cross_entropy\")\n",
        "# loss\n",
        "loss = tf.reduce_sum(cross_entropy, name=\"cross_entropy_sum\")\n",
        "# gradient pipeline\n",
        "grads_and_vars = opt.compute_gradients(loss)\n",
        "grads_and_vars = [(tf.clip_by_norm(g, param.max_grad_norm) if g is not None else g, v) for g,v in grads_and_vars]\n",
        "grads_and_vars = [(add_gradient_noise(g) if g is not None else g, v) for g,v in grads_and_vars]\n",
        "nil_grads_and_vars = []\n",
        "for g, v in grads_and_vars:\n",
        "    if v.name in _nil_vars:\n",
        "        nil_grads_and_vars.append((zero_nil_slot(g), v))\n",
        "    else:\n",
        "        nil_grads_and_vars.append((g, v))\n",
        "train_op = opt.apply_gradients(nil_grads_and_vars, name=\"train_op\")\n",
        "\n",
        "predict_op = tf.argmax(logits, 1, name=\"predict_op\")\n",
        "predict_proba_op = tf.nn.softmax(logits, name=\"predict_proba_op\")\n",
        "predict_log_proba_op = tf.log(predict_proba_op, name=\"predict_log_proba_op\")\n",
        "\n",
        "batch_size = param.batch_size\n",
        "\n",
        "batches = zip(range(0, n_train-batch_size, batch_size), range(batch_size, n_train, batch_size))\n",
        "batches = [(start, end) for start, end in batches]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YU6ZI7qHU5_2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Start Training and Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "MPPKx0Xr9Pm6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "#     model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, param.embedding_size, session=sess,\n",
        "#                    hops=param.hops, max_grad_norm=param.max_grad_norm)\n",
        "    for t in range(1, param.epochs+1):\n",
        "        # Stepped learning rate\n",
        "        if t - 1 <= param.anneal_stop_epoch:\n",
        "            anneal = 2.0 ** ((t - 1) // param.anneal_rate)\n",
        "        else:\n",
        "            anneal = 2.0 ** (param.anneal_stop_epoch // param.anneal_rate)\n",
        "        lr = param.learning_rate / anneal\n",
        "\n",
        "        np.random.shuffle(batches)\n",
        "        total_cost = 0.0\n",
        "        for start, end in batches:\n",
        "            s = trainS[start:end]\n",
        "            q = trainQ[start:end]\n",
        "            a = trainA[start:end]\n",
        "            cost_t, _ = sess.run([loss, train_op], \n",
        "                                 feed_dict={_stories: s, _queries: q, _answers: a, _lr: lr})\n",
        "#             cost_t = model.batch_fit(s, q, a, lr)\n",
        "            total_cost += cost_t\n",
        "\n",
        "        if t % param.evaluation_interval == 0:\n",
        "            train_preds = []\n",
        "            for start in range(0, n_train, batch_size):\n",
        "                end = start + batch_size\n",
        "                s = trainS[start:end]\n",
        "                q = trainQ[start:end]\n",
        "#                 pred = model.predict(s, q)\n",
        "                pred = sess.run(predict_op, feed_dict={_stories: s, _queries: q})\n",
        "                train_preds += list(pred)\n",
        "            \n",
        "            val_preds = sess.run(predict_op, feed_dict={_stories: valS, _queries: valQ})\n",
        "#             val_preds = model.predict(valS, valQ)\n",
        "            train_acc = metrics.accuracy_score(np.array(train_preds), train_labels)\n",
        "            val_acc = metrics.accuracy_score(val_preds, val_labels)\n",
        "\n",
        "            print('-----------------------')\n",
        "            print('Epoch', t)\n",
        "            print('Total Cost:', total_cost)\n",
        "            print('Training Accuracy:', train_acc)\n",
        "            print('Validation Accuracy:', val_acc)\n",
        "            print('-----------------------')\n",
        "    test_preds = sess.run(predict_op, feed_dict={_stories: testS, _queries: testQ})\n",
        "#     test_preds = model.predict(testS, testQ)\n",
        "    test_acc = metrics.accuracy_score(test_preds, test_labels)\n",
        "    print(\"Testing Accuracy:\", test_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYxSciVidvAr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Homework\n",
        "\n",
        "1. In the original paper, there is a training scheme called linear start (LS) training:  \n",
        "The softmax in each memory layer is removed, making the model entirely linear except for the final softmax for answer prediction. When the validation loss stopped decreasing, the softmax layers were re-inserted and training recommenced.  \n",
        "Pleas implement this training scheme.   \n",
        "(hint: put tf.nn.softmax and tf.identity to tf.cond with a boolean scalar. Once the validation loss stopped decreasing, negate the boolean scalar.)\n",
        "2. Implement Layer-wise (RNN-like) weight tying scheme. Compare the performace between adjacent share weight scheme and Layer-wise (RNN-like) weight tying scheme.\n",
        "3. Use bag-of-word instead of positioin encoding for senetence representations. Compare the performace between bag-of-word and positioin encoding."
      ]
    }
  ]
}